{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed94bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c27ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.1056\n",
      "Epoch 2/100, Loss: 0.0271\n",
      "Epoch 3/100, Loss: 0.0177\n",
      "Epoch 4/100, Loss: 0.0127\n",
      "Epoch 5/100, Loss: 0.0093\n",
      "Epoch 6/100, Loss: 0.0071\n",
      "Epoch 7/100, Loss: 0.0057\n",
      "Epoch 8/100, Loss: 0.0048\n",
      "Epoch 9/100, Loss: 0.0041\n",
      "Epoch 10/100, Loss: 0.0032\n",
      "Epoch 11/100, Loss: 0.0025\n",
      "Epoch 12/100, Loss: 0.0022\n",
      "Epoch 13/100, Loss: 0.0015\n",
      "Epoch 14/100, Loss: 0.0013\n",
      "Epoch 15/100, Loss: 0.0012\n",
      "Epoch 16/100, Loss: 0.0010\n",
      "Epoch 17/100, Loss: 0.0009\n",
      "Epoch 18/100, Loss: 0.0007\n",
      "Epoch 19/100, Loss: 0.0006\n",
      "Epoch 20/100, Loss: 0.0006\n",
      "Epoch 21/100, Loss: 0.0005\n",
      "Epoch 22/100, Loss: 0.0004\n",
      "Epoch 23/100, Loss: 0.0004\n",
      "Epoch 24/100, Loss: 0.0003\n",
      "Epoch 25/100, Loss: 0.0002\n",
      "Epoch 26/100, Loss: 0.0002\n",
      "Epoch 27/100, Loss: 0.0002\n",
      "Epoch 28/100, Loss: 0.0002\n",
      "Epoch 29/100, Loss: 0.0001\n",
      "Epoch 30/100, Loss: 0.0001\n",
      "Epoch 31/100, Loss: 0.0001\n",
      "Epoch 32/100, Loss: 0.0001\n",
      "Epoch 33/100, Loss: 0.0001\n",
      "Epoch 34/100, Loss: 0.0000\n",
      "Epoch 35/100, Loss: 0.0000\n",
      "Epoch 36/100, Loss: 0.0000\n",
      "Epoch 37/100, Loss: 0.0000\n",
      "Epoch 38/100, Loss: 0.0000\n",
      "Epoch 39/100, Loss: 0.0000\n",
      "Epoch 40/100, Loss: 0.0000\n",
      "Epoch 41/100, Loss: 0.0000\n",
      "Epoch 42/100, Loss: 0.0000\n",
      "Epoch 43/100, Loss: 0.0000\n",
      "Epoch 44/100, Loss: 0.0000\n",
      "Epoch 45/100, Loss: 0.0000\n",
      "Epoch 46/100, Loss: 0.0000\n",
      "Epoch 47/100, Loss: 0.0000\n",
      "Epoch 48/100, Loss: 0.0000\n",
      "Epoch 49/100, Loss: 0.0000\n",
      "Epoch 50/100, Loss: 0.0000\n",
      "Epoch 51/100, Loss: 0.0000\n",
      "Epoch 52/100, Loss: 0.0000\n",
      "Epoch 53/100, Loss: 0.0000\n",
      "Epoch 54/100, Loss: 0.0000\n",
      "Epoch 55/100, Loss: 0.0000\n",
      "Epoch 56/100, Loss: 0.0000\n",
      "Epoch 57/100, Loss: 0.0000\n",
      "Epoch 58/100, Loss: 0.0000\n",
      "Epoch 59/100, Loss: 0.0000\n",
      "Epoch 60/100, Loss: 0.0000\n",
      "Epoch 61/100, Loss: 0.0000\n",
      "Epoch 62/100, Loss: 0.0000\n",
      "Epoch 63/100, Loss: 0.0000\n",
      "Epoch 64/100, Loss: 0.0000\n",
      "Epoch 65/100, Loss: 0.0000\n",
      "Epoch 66/100, Loss: 0.0000\n",
      "Epoch 67/100, Loss: 0.0000\n",
      "Epoch 68/100, Loss: 0.0000\n",
      "Epoch 69/100, Loss: 0.0000\n",
      "Epoch 70/100, Loss: 0.0000\n",
      "Epoch 71/100, Loss: 0.0000\n",
      "Epoch 72/100, Loss: 0.0000\n",
      "Epoch 73/100, Loss: 0.0000\n",
      "Epoch 74/100, Loss: 0.0000\n",
      "Epoch 75/100, Loss: 0.0000\n",
      "Epoch 76/100, Loss: 0.0000\n",
      "Epoch 77/100, Loss: 0.0000\n",
      "Epoch 78/100, Loss: 0.0000\n",
      "Epoch 79/100, Loss: 0.0000\n",
      "Epoch 80/100, Loss: 0.0000\n",
      "Epoch 81/100, Loss: 0.0000\n",
      "Epoch 82/100, Loss: 0.0000\n",
      "Epoch 83/100, Loss: 0.0000\n",
      "Epoch 84/100, Loss: 0.0000\n",
      "Epoch 85/100, Loss: 0.0000\n",
      "Epoch 86/100, Loss: 0.0000\n",
      "Epoch 87/100, Loss: 0.0000\n",
      "Epoch 88/100, Loss: 0.0000\n",
      "Epoch 89/100, Loss: 0.0000\n",
      "Epoch 90/100, Loss: 0.0000\n",
      "Epoch 91/100, Loss: 0.0000\n",
      "Epoch 92/100, Loss: 0.0000\n",
      "Epoch 93/100, Loss: 0.0000\n",
      "Epoch 94/100, Loss: 0.0000\n",
      "Epoch 95/100, Loss: 0.0000\n",
      "Epoch 96/100, Loss: 0.0000\n",
      "Epoch 97/100, Loss: 0.0000\n",
      "Epoch 98/100, Loss: 0.0000\n",
      "Epoch 99/100, Loss: 0.0002\n",
      "Epoch 100/100, Loss: 0.0009\n",
      "Predicted Frustration Level: 0.9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT01\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('memory_match_frustration_data_200_samples.csv')\n",
    "\n",
    "# Custom Dataset Class\n",
    "class MemoryMatchDataset(Dataset):\n",
    "    def __init__(self, dataframe, sequence_length=10):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data = dataframe[['reaction_time (sec)', 'incorrect_attempts', 'repeated_incorrect', \n",
    "                               'negative_penalty', 'score']].values.astype(np.float32)\n",
    "\n",
    "        # Simulated frustration levels (heuristic for demo)\n",
    "        self.targets = ((dataframe['incorrect_attempts'] + dataframe['repeated_incorrect'] + \n",
    "                         dataframe['negative_penalty']) / (dataframe['move_number'] + 1)).values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.sequence_length]\n",
    "        y = self.targets[idx + self.sequence_length - 1]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# GRU Model\n",
    "class FrustrationGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FrustrationGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "INPUT_SIZE = 5\n",
    "HIDDEN_SIZE = 32\n",
    "OUTPUT_SIZE = 1\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Prepare Dataset and DataLoader\n",
    "dataset = MemoryMatchDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = FrustrationGRU(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs.squeeze(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Example Inference (Predict Frustration)\n",
    "with torch.no_grad():\n",
    "    sample_seq = torch.tensor(dataset[0][0]).unsqueeze(0)  \n",
    "    predicted_frustration = model(sample_seq)\n",
    "    print(f\"Predicted Frustration Level: {predicted_frustration.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
